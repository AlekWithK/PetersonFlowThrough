{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccac275a-9062-4606-960c-21da23c50c6c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e005524-4700-417b-973a-8635f517961b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "import math\n",
    "import geopy.distance\n",
    "\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f823e31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahernes\\AppData\\Local\\Temp\\1\\ipykernel_14868\\3160088194.py:2: DtypeWarning: Columns (18,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  (pd.read_csv('Merges/PETERSON_CLEANED.csv')\n"
     ]
    }
   ],
   "source": [
    "# Convert cleaned CSV from Alteryx to parquet\n",
    "(pd.read_csv('Merges/PETERSON_CLEANED.csv')\n",
    " .astype({'mark': str, 'file': str, 'dataset': str, 'comment': str})\n",
    " .to_parquet('Merges/PETERSON_CLEANED.parquet', engine='auto'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff46b0b",
   "metadata": {},
   "source": [
    "## 1995-2012/MIDAS 1995-2000 Aggregation\n",
    "Encompasses all datasets in 'MIDAS 1995-2000' directory parsing through '.001' files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9cbdd6f-952c-43ec-8c1b-2b5e932463c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 01040657.001\n",
      "Processing: 01050741.001\n",
      "Processing: 02290721.001\n",
      "Processing: 03020914.001\n",
      "Processing: 03290742.001\n",
      "Processing: 03290809.001\n",
      "Processing: 03291311.001\n",
      "Processing: 03291341.001\n",
      "Processing: 04040717.001\n",
      "Processing: 04040750.001\n",
      "Processing: 04040756.001\n",
      "Processing: 04060804.001\n",
      "Processing: 04060930.001\n",
      "Processing: 05180748.001\n",
      "Processing: 02080756.001\n",
      "Processing: 02100733.001\n",
      "Processing: 02250643.001\n",
      "ERROR: file structure not yet supported\n",
      "Processing: 03090617.001\n",
      "ERROR: file structure not yet supported\n",
      "Processing: 03160520.001\n",
      "ERROR: Error tokenizing data. C error: Expected 38 fields in line 270, saw 40\n",
      "\n",
      "Processing: 03170629.001\n",
      "ERROR: Error tokenizing data. C error: Expected 38 fields in line 2005, saw 40\n",
      "\n",
      "Processing: 11290520.001\n",
      "Processing: 11290604.001\n",
      "Processing: 11290633.001\n",
      "Processing: 11291104.001\n",
      "Processing: 11300658.001\n",
      "Processing: 04040505.001\n",
      "Processing: 04040628.001\n",
      "Processing: 04041047.001\n",
      "Processing: 04050621.001\n",
      "Processing: 04070708.001\n",
      "Processing: 04070822.001\n",
      "Processing: 04071324.001\n",
      "Processing: 04180755.001\n",
      "Processing: 04180944.001\n",
      "Processing: 04180950.001\n",
      "Processing: 04180954.001\n",
      "Processing: 04181036.001\n",
      "Processing: 04181058.001\n",
      "Processing: 95APR18.001\n",
      "Processing: 04190957.001\n",
      "Processing: 04240807.001\n",
      "Processing: 04240930.001\n",
      "Processing: 04241326.001\n",
      "Processing: 08160523.001\n",
      "Processing: 08160949.001\n",
      "Processing: 02070617.001\n",
      "Processing: 02070958.001\n",
      "Processing: 02071339.001\n",
      "Processing: 02071340.001\n",
      "Processing: 02071344.001\n",
      "Processing: 02130734.001\n",
      "Processing: 02150810.001\n",
      "Processing: 02150928.001\n",
      "Processing: 02150934.001\n",
      "Processing: 02151346.001\n",
      "Processing: 02220826.001\n",
      "Processing: 02220848.001\n",
      "Processing: 02280813.001\n",
      "Processing: 02281042.001\n",
      "Processing: 02281432.001\n",
      "Processing: 01130739.001\n",
      "Processing: 01130745.001\n",
      "Processing: 01130853.001\n",
      "Processing: 01130859.001\n",
      "Processing: 01130903.001\n",
      "Processing: 01130906.001\n",
      "Processing: 01131444.001\n",
      "Processing: 01190603.001\n",
      "Processing: 01241000.001\n",
      "Processing: 01241349.001\n",
      "Processing: 07180620.001\n",
      "Processing: 07181017.001\n",
      "Processing: 07181041.001\n",
      "Processing: 07181641.001\n",
      "Processing: 06130520.001\n",
      "Processing: 06130628.001\n",
      "Processing: 06131024.001\n",
      "Processing: 03160604.001\n",
      "Processing: 03160645.001\n",
      "Processing: 03160831.001\n",
      "Processing: 03161252.001\n",
      "Processing: 03230717.001\n",
      "Processing: 03231216.001\n",
      "Processing: 03300711.001\n",
      "Processing: 03300928.001\n",
      "Processing: 05020507.001\n",
      "Processing: 05020511.001\n",
      "Processing: 05020619.001\n",
      "Processing: 05020704.001\n",
      "Processing: 05020955.001\n",
      "Processing: 05090838.001\n",
      "Processing: 05090957.001\n",
      "Processing: 05091346.001\n",
      "Processing: 05160904.001\n",
      "Processing: 10230607.001\n",
      "Processing: 10230720.001\n",
      "Processing: 10240841.001\n",
      "Processing: 10250915.001\n",
      "Processing: 09210642.001\n",
      "Processing: 09220801.001\n",
      "Processing: 04030619.001\n",
      "Processing: 04180932.001\n",
      "Processing: 04230827.001\n",
      "Processing: 08130625.001\n",
      "Processing: 12170614.001\n",
      "Processing: 02010925.001\n",
      "Processing: 02060527.001\n",
      "Processing: 02060615.001\n",
      "Processing: 02130832.001\n",
      "Processing: 02210916.001\n",
      "Processing: 01160712.001\n",
      "Processing: 01160840.001\n",
      "Processing: 07170511.001\n",
      "Processing: 07171005.001\n",
      "Processing: 06120611.001\n",
      "Processing: 03011029.001\n",
      "Processing: 03060624.001\n",
      "Processing: 03140816.001\n",
      "Processing: 03141206.001\n",
      "Processing: 03260903.001\n",
      "Processing: 05010613.001\n",
      "Processing: 05011018.001\n",
      "Processing: 11130635.001\n",
      "Processing: 10160633.001\n",
      "Processing: 04010614.001\n",
      "Processing: 04100905.001\n",
      "Processing: 04220625.001\n",
      "Processing: 04300922.001\n",
      "Processing: 08050623.001\n",
      "Processing: 08051151.001\n",
      "Processing: 08260712.001\n",
      "Processing: 12090705.001\n",
      "Processing: 02140826.001\n",
      "Processing: 02191026.001\n",
      "Processing: 02260624.001\n",
      "Processing: 01130803.001\n",
      "Processing: 01280617.001\n",
      "Processing: 01291355.001\n",
      "Processing: 07150618.001\n",
      "Processing: 06100552.001\n",
      "Processing: 03060827.001\n",
      "Processing: 03111038.001\n",
      "Processing: 03170823.001\n",
      "Processing: 05140613.001\n",
      "Processing: 11050523.001\n",
      "Processing: 11060608.001\n",
      "Processing: 10070556.001\n",
      "Processing: 09090522.001\n",
      "Processing: 09091029.001\n",
      "Processing: 04020952.001\n",
      "Processing: 04090736.001\n",
      "Processing: 04090933.001\n",
      "Processing: 04140628.001\n",
      "Processing: 04210839.001\n",
      "Processing: 04281018.001\n",
      "Processing: 08110623.001\n",
      "Processing: 12080623.001\n",
      "Processing: 02110932.001\n",
      "Processing: 02180511.001\n",
      "Processing: 02180734.001\n",
      "Processing: 02181216.001\n",
      "Processing: 02191307.001\n",
      "Processing: 02260955.001\n",
      "Processing: 01060553.001\n",
      "Processing: 01200842.001\n",
      "Processing: 07210611.001\n",
      "Processing: 07210750.001\n",
      "Processing: 07210755.001\n",
      "Processing: 07210802.001\n",
      "Processing: 07211113.001\n",
      "Processing: 06020919.001\n",
      "Processing: 06170602.001\n",
      "Processing: 03050822.001\n",
      "Processing: 03051220.001\n",
      "Processing: 03120922.001\n",
      "Processing: 03171011.001\n",
      "Processing: 03171030.001\n",
      "Processing: 03271113.001\n",
      "Processing: 03271532.001\n",
      "Processing: 05050817.001\n",
      "Processing: 05140952.001\n",
      "Processing: 05141006.001\n",
      "Processing: 05190609.001\n",
      "Processing: 11100617.001\n",
      "Processing: 10150732.001\n",
      "Processing: 10150918.001\n",
      "Processing: 10150951.001\n",
      "Processing: 10151005.001\n",
      "Processing: 10152100.001\n",
      "Processing: 09010520.001\n",
      "Processing: 09010524.001\n",
      "Processing: 09010613.001\n",
      "Processing: 04060925.001\n",
      "Processing: 04130518.001\n",
      "Processing: 04140652.001\n",
      "Processing: 08170747.001\n",
      "Processing: 08170947.001\n",
      "Processing: 08170950.001\n",
      "Processing: 08171011.001\n",
      "Processing: 08171034.001\n",
      "Processing: 08171221.001\n",
      "Processing: 08171306.001\n",
      "Processing: 08182027.001\n",
      "Processing: 12160810.001\n",
      "Processing: 02100531.001\n",
      "Processing: 02230857.001\n",
      "Processing: 01210611.001\n",
      "Processing: 06070809.001\n",
      "Processing: 06071000.001\n",
      "Processing: 06071217.001\n",
      "Processing: 03090611.001\n",
      "Processing: 03240732.001\n",
      "Processing: 05070606.001\n",
      "Processing: 05070700.001\n",
      "Processing: 11100602.001\n",
      "Processing: 11110914.001\n",
      "Processing: 10190733.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahernes\\AppData\\Local\\Temp\\1\\ipykernel_14868\\989357820.py:66: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_midas_master = pd.concat(master_list, axis=0).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets Aggregated: 213\n",
      "Errors: 4\n",
      "['02250643.001', '03090617.001', '03160520.001', '03170629.001']\n"
     ]
    }
   ],
   "source": [
    "# 232 '.001' files in MIDAS 1995-2000\n",
    "err_list = []\n",
    "master_list = []\n",
    "file_list = []\n",
    "midas_cols = ['date', 'time', 'lat', 'lon', 'depth_md', 'temp_air_md', 'wind_spd_md', 'wind_dir_md', 'barometer_md', 'bow_temp_md', 'salinity_md', 'sus_part_matter_md', \n",
    "        'photic_depth_md', 'chlor_md', 'neph_md', 'fluor_switch_md', 'event_mark_md', 'comment_num_md', '_drop', '_drop2']\n",
    "\n",
    "def convert_midas_datetime(df):\n",
    "    # Some entries contain 99-99-9999 for date and 99:99:99 for time\n",
    "    df = df.rename(columns={'date_time': 'datetime'})\n",
    "    df.datetime = pd.to_datetime(df.datetime, errors='coerce')\n",
    "    df = df[pd.notnull(df.datetime)]\n",
    "    return df\n",
    "\n",
    "def convert_dms_decimal(value):\n",
    "    tag = value[-1]\n",
    "    direction = {'N':1, 'S':-1, 'E': 1, 'W':-1}\n",
    "\n",
    "    second = int(value[-3:-1]) / 3600\n",
    "    minute = int(value[-6:-4]) / 60\n",
    "    degree = int(str(float(value[:-1]) / 100).split('.')[0])\n",
    "\n",
    "    return (degree + minute + second) * direction[tag]\n",
    "\n",
    "\n",
    "def convert_midas_latlon(df):\n",
    "    df.lat = df.lat.apply(convert_dms_decimal)\n",
    "    df.lon = df.lon.apply(convert_dms_decimal)\n",
    "\n",
    "    # Filter out error values of 99 and 999 respectively\n",
    "    df = df[abs(df.lat) < 99]\n",
    "    df = df[abs(df.lon) < 999]\n",
    "    return df\n",
    "\n",
    "\n",
    "# Iterate over all files in 1995-2000\n",
    "def read_midas_file():\n",
    "    count = 0\n",
    "    test_limit = math.inf\n",
    "    for root, _, files in os.walk('Flow Through/1995-2012/MIDAS 1995-2000'):    \n",
    "        for file in files:\n",
    "            if file.endswith('.001') and file not in file_list:\n",
    "                try:\n",
    "                    print(f'Processing: {file}')\n",
    "                    with warnings.catch_warnings(): # Catching a warning about parse_dates being retired\n",
    "                        warnings.simplefilter(\"ignore\")\n",
    "                        file_list.append(file)\n",
    "                        df = pd.read_csv(os.path.join(root, file), encoding='unicode_escape', names=midas_cols, header=None, parse_dates=[['date', 'time']]).drop(columns=['_drop', '_drop2'])\n",
    "                        df['sus_part_matter_md'] = df['sus_part_matter_md'].astype(str)\n",
    "                        df['photic_depth_md'] = df['photic_depth_md'].astype(str)\n",
    "                        df = convert_midas_datetime(df)\n",
    "                        df = convert_midas_latlon(df)\n",
    "                        df['dataset'] = 'MIDAS'\n",
    "                        df['file'] = file                   \n",
    "                        master_list.append(df)\n",
    "\n",
    "                    count += 1\n",
    "                    if count == test_limit: return\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    err_list.append(file)\n",
    "                    file_list.append(file)\n",
    "                    print(f'ERROR: {e}')\n",
    "                \n",
    "read_midas_file()\n",
    "df_midas_master = pd.concat(master_list, axis=0).reset_index(drop=True)\n",
    "print(f'Datasets Aggregated: {len(master_list)}')\n",
    "print(f'Errors: {len(err_list)}') \n",
    "print(err_list)\n",
    "\n",
    "df_midas_master.to_csv('Merges/midas_test_merge.csv')\n",
    "df_midas_master.to_parquet('Merges/midas_test_merge.parquet', engine='auto')       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbefe03",
   "metadata": {},
   "source": [
    "## 1995-2012/MOPED Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc05290a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 00166.txt\n",
      "Processing: 00193.txt\n",
      "Processing: 00223.txt\n",
      "Processing: 00249dat.txt\n",
      "Processing: 00251dat.txt\n",
      "Processing: 00284.txt\n",
      "Processing: 00286.txt\n",
      "Processing: 00312dat.txt\n",
      "Processing: 00314dat.txt\n",
      "Processing: 00347dat.txt\n",
      "Processing: 00349dat.txt\n",
      "Processing: 12193dat.csv\n",
      "Processing: 12199dat.txt\n",
      "Processing: 00221b.txt\n",
      "Processing: 00195a.txt\n",
      "Processing: 00165combo.txt\n",
      "Processing: test165.txt\n",
      "Processing: 00167.txt\n",
      "Processing: 01037bdat.txt\n",
      "Processing: 01039cdat.txt\n",
      "Processing: 01046dat.txt\n",
      "Processing: 01053dat.txt\n",
      "Processing: 01057dat.txt\n",
      "Processing: 01059dat.txt\n",
      "Processing: 01066adat.txt\n",
      "Processing: 01075dat.txt\n",
      "Processing: 01086dat.txt\n",
      "Processing: 01088dat.txt\n",
      "Processing: 01100dat.txt\n",
      "Processing: 01106dat.txt\n",
      "Processing: 01114dat.txt\n",
      "Processing: 01116dat.txt\n",
      "Processing: 01144dat.txt\n",
      "Processing: 01170dat.txt\n",
      "Processing: 01172bdat.txt\n",
      "Processing: 01198dat.txt\n",
      "Processing: 01200dat.txt\n",
      "Processing: 01254dat.txt\n",
      "Processing: 01289dat.txt\n",
      "Processing: 01291bdat.txt\n",
      "Processing: 01331dat.txt\n",
      "Processing: 01352dat.txt\n",
      "Processing: 01354dat.txt\n",
      "Processing: 01093dat.txt\n",
      "Processing: 01256dat.txt\n",
      "Processing: 01333dat.txt\n",
      "Processing: 02022dat.txt\n",
      "Processing: 02024dat.txt\n",
      "Processing: 02039dat.txt\n",
      "Processing: 02071bdat.txt\n",
      "Processing: 02078bdat.txt\n",
      "Processing: 02086dat.txt\n",
      "Processing: 02106dat.txt\n",
      "Processing: 02112dat.txt\n",
      "Processing: 02116dat.txt\n",
      "Processing: 02127dat.txt\n",
      "Processing: 02133dat.txt\n",
      "Processing: 02155bdat.txt\n",
      "Processing: 02197bdat.txt\n",
      "Processing: 02232dat.txt\n",
      "Processing: 02317adat.txt\n",
      "Processing: 02344dat.txt\n",
      "Processing: 03007dat.txt\n",
      "Processing: 03041dat.txt\n",
      "Processing: 03050dat.txt\n",
      "Processing: 03055dat.txt\n",
      "Processing: 03063dat.txt\n",
      "Processing: 03071dat.txt\n",
      "Processing: 03077adat.txt\n",
      "Processing: 03086dat.txt\n",
      "Processing: 03091dat.txt\n",
      "Processing: 03098dat.txt\n",
      "Processing: 03107dat.txt\n",
      "Processing: 03113dat.txt\n",
      "Processing: 03121dat.txt\n",
      "Processing: 03140dat.txt\n",
      "Processing: 03168dat.txt\n",
      "Processing: 03196dat.txt\n",
      "Processing: 03224dat.txt\n",
      "Processing: 03239dat.txt\n",
      "Processing: 03252dat.txt\n",
      "Processing: 03288dat.txt\n",
      "Processing: 03322dat.txt\n",
      "Processing: 03350dat.txt\n",
      "Processing: 04174dat.txt\n",
      "Processing: 2004_salinities.txt\n",
      "Processing: 04013dat.txt\n",
      "Processing: 04058dat.txt\n",
      "Processing: 04069dat.txt\n",
      "Processing: 04084dat.txt\n",
      "Processing: 04090dat.txt\n",
      "Processing: 04105dat.txt\n",
      "Processing: 04111dat.txt\n",
      "Processing: 04117dat.txt\n",
      "Processing: 04139dat.txt\n",
      "Processing: 04209dat.txt\n",
      "Processing: 04237dat.txt\n",
      "Processing: 04252dat.txt\n",
      "Processing: 04258dat.txt\n",
      "Processing: 04308dat.txt\n",
      "Processing: 04349dat.txt\n",
      "Processing: 05054_salinitiesforsteve.txt\n",
      "Processing: salinities_forsteve.txt\n",
      "Processing: 05011dat.txt\n",
      "Processing: 06010dat.txt\n",
      "Processing: ctd_moped.txt\n",
      "Processing: 06045dat.txt\n",
      "Processing: 06052dat.txt\n",
      "Processing: 06074dat.txt\n",
      "Processing: 06080dat.txt\n",
      "Processing: 06095dat.txt\n",
      "Processing: 06101dat.txt\n",
      "Processing: 06111dat.txt\n",
      "Processing: 06116dat.txt\n",
      "Processing: 06129dat.txt\n",
      "Processing: 06192dat.txt\n",
      "Processing: 06214dat.txt\n",
      "Processing: 06227dat.txt\n",
      "Processing: 06243dat.txt\n",
      "Processing: 06255dat.txt\n",
      "Processing: 06284dat.txt\n",
      "Processing: 06290dat.txt\n",
      "Processing: 06318dat.txt\n",
      "Processing: 06346dat.txt\n",
      "Processing: important note on 2007 moped data.txt\n",
      "Processing: 07009dat.txt\n",
      "Processing: 07037dat.txt\n",
      "Processing: 07040dat.txt\n",
      "Processing: 07065dat.txt\n",
      "Processing: 07071dat.txt\n",
      "Processing: 07087adat.txt\n",
      "Processing: 07093dat.txt\n",
      "Processing: 07100dat.txt\n",
      "Processing: 07113dat.txt\n",
      "Processing: 07254dat.txt\n",
      "Processing: 07292dat.txt\n",
      "Processing: 07296dat.txt\n",
      "Processing: 07318dat.txt\n",
      "Processing: 08010dat.txt\n",
      "Processing: please read_08032.txt\n",
      "Processing: 08043dat.txt\n",
      "Processing: 08060dat.txt\n",
      "Processing: 08071dat.txt\n",
      "Processing: 08091dat.txt\n",
      "Processing: 08105dat.txt\n",
      "Processing: 08119dat.txt\n",
      "Processing: 08127dat.txt\n",
      "Processing: 08150dat.txt\n",
      "Processing: 08169dat.txt\n",
      "Processing: 08192dat.txt\n",
      "Processing: 08197dat.txt\n",
      "Processing: 08225dat.txt\n",
      "Processing: 08232dat.txt\n",
      "Processing: 08253dat.txt\n",
      "Processing: 08260dat.txt\n",
      "Processing: 08289dat.txt\n",
      "Processing: 08310dat.txt\n",
      "Processing: 08323dat.txt\n",
      "Processing: 08351dat.txt\n",
      "Processing: 09013dat.txt\n",
      "Processing: 09041dat.txt\n",
      "Processing: 09048dat.txt\n",
      "Processing: 09064dat.txt\n",
      "Processing: 09071dat.txt\n",
      "Processing: 09097dat.txt\n",
      "Processing: 09106dat.txt\n",
      "Processing: 09125dat.txt\n",
      "Processing: 09139dat.txt\n",
      "Processing: 09174dat.txt\n",
      "Processing: 09196dat.txt\n",
      "Processing: 09202dat.txt\n",
      "Processing: 09225dat.txt\n",
      "Processing: 09239dat.txt\n",
      "Processing: 09265dat.txt\n",
      "Processing: 09302dat.txt\n",
      "Processing: 09322dat.txt\n",
      "Processing: 09337dat.txt\n",
      "Processing: 09140dat.txt\n",
      "Processing: note.txt\n",
      "Processing: 10005dat.txt\n",
      "Processing: copy of 10005dat.txt\n",
      "Processing: 10026dat.txt\n",
      "Processing: 10040dat.txt\n",
      "Processing: 10054dat.txt\n",
      "Processing: 10068dat.txt\n",
      "Processing: 10083dat.txt\n",
      "Processing: 10089dat.txt\n",
      "Processing: 10097dat.txt\n",
      "Processing: 10103dat.txt\n",
      "Processing: 10113dat.txt\n",
      "Processing: 10127dat.txt\n",
      "Processing: 10140 important note!!!.txt\n",
      "Processing: 10158dat.txt\n",
      "Processing: 10166dat.txt\n",
      "Processing: 10187dat.txt\n",
      "Processing: 10194dat.txt\n",
      "Processing: 10215dat.txt\n",
      "Processing: 10229dat.txt\n",
      "Processing: 10244dat.txt\n",
      "Processing: 10257dat.txt\n",
      "Processing: 10274dat.txt\n",
      "Processing: 10299dat.txt\n",
      "Processing: 10319dat.txt\n",
      "Processing: 10350dat.txt\n",
      "Processing: 11040dat.txt\n",
      "Processing: 11088dat.txt\n",
      "Processing: 11098dat.txt\n",
      "Processing: 11102dat.txt\n",
      "Processing: 11109dat.txt\n",
      "Processing: 11118dat.txt\n",
      "Processing: 11131dat.txt\n",
      "Processing: 11165dat.txt\n",
      "Processing: 11175dat.txt\n",
      "Processing: 11193dat.txt\n",
      "Processing: 11203dat.txt\n",
      "Processing: 11228dat.txt\n",
      "Processing: 11235dat.txt\n",
      "Processing: 11279dat.txt\n",
      "Processing: 11291dat.txt\n",
      "Processing: 11308dat.txt\n",
      "Processing: 11319dat.txt\n",
      "Processing: 11347dat.txt\n",
      "Processing: 12010dat.txt\n",
      "Processing: 12031dat.txt\n",
      "Processing: 12038dat.txt\n",
      "Processing: 12061dat.txt\n",
      "Processing: 12080dat.txt\n",
      "Processing: 12090dat.txt\n",
      "Processing: 12102dat.txt\n",
      "Processing: 12111dat.txt\n",
      "Processing: 12118dat.txt\n",
      "Processing: 12136dat.txt\n",
      "Processing: 12144dat.txt\n",
      "Processing: 12164dat.txt\n",
      "Processing: 12171dat.txt\n",
      "Processing: 12220dat.txt\n",
      "Processing: 12240dat.txt\n",
      "Processing: 12251dat.txt\n",
      "Processing: 12255dat.txt\n",
      "Processing: 12277dat.txt\n",
      "Processing: 12284dat.txt\n",
      "Processing: 12311dat.txt\n",
      "Processing: 12339dat.txt\n",
      "Datasets Aggregated: 243\n",
      "Errors: 0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "err_list = []\n",
    "master_list = []\n",
    "file_list = []\n",
    "moped_cols = ['datetime', 'lat deg', 'lat min', 'lon deg', 'lon min', 'utm_north', 'utm_east', 'depth_mp', 'heading_mag_mp',\n",
    "              'heading_gps_mp', 'speed_gps_mp', 'speed_water_mp', 'speed_wind_mp', 'wind_dir_mp', 'barometer_mp', 'temp_air_mp', 'spare_mp', 'chlor_mp',\n",
    "              'turb_mp', 'temp_water_mp', 'salinity_mp', 'temp_bow_mp', 'mark_mp']\n",
    "\n",
    "def convert_moped_datetime(df):\n",
    "    df.datetime = pd.to_datetime(df.datetime, format='%m/%d/%Y %H:%M:%S', errors='coerce')\n",
    "    df = df[pd.notnull(df.datetime)]\n",
    "    return df\n",
    "\n",
    "def convert_moped_latlon(df):\n",
    "    # Just reassigning lat deg and lat min (col 1 and 2) rather than creating new columns and reordering\n",
    "    df['lat deg'] = df['lat deg'].astype(int) + df['lat min'].astype(float) / 60\n",
    "    df['lat min'] = df['lon deg'].astype(int) + df['lon min'].astype(float) / 60\n",
    "    df = df.drop(columns=['lon deg', 'lon min']).rename(columns={'lat deg': 'lat', 'lat min': 'lon'})\n",
    "    return df\n",
    "\n",
    "def read_moped_file():\n",
    "    count = 0\n",
    "    test_limit = math.inf\n",
    "    for root, _, files in os.walk('Flow Through/1995-2012/MOPED'):    \n",
    "        for file in files:\n",
    "            file = file.lower() # Some files end in '.TXT'\n",
    "            if (file.endswith('.txt') or file.endswith('.csv')) and file[:5] not in file_list and \"log\" not in file:\n",
    "            #if file == '05054_salinitiesforsteve.txt':\n",
    "                try:\n",
    "                    print(f'Processing: {file}')\n",
    "                    file_start = file[:5]\n",
    "                    file_list.append(file_start)\n",
    "                    df = pd.read_csv(os.path.join(root, file), names=moped_cols, header=None).drop(columns=['utm_north', 'utm_east'])\n",
    "                    df.mark_mp = df.mark_mp.astype(str)\n",
    "                    df = convert_moped_datetime(df)\n",
    "                    df = convert_moped_latlon(df)\n",
    "                    df['dataset'] = 'MOPED'\n",
    "                    df['file'] = file\n",
    "                    master_list.append(df)\n",
    "\n",
    "                    count += 1\n",
    "                    if count == test_limit: return\n",
    "\n",
    "                except Exception as e:\n",
    "                    err_list.append(file)\n",
    "                    file_list.append(file)\n",
    "                    print(f'ERROR: {e}')\n",
    "\n",
    "    \n",
    "read_moped_file()\n",
    "df_moped_master = pd.concat(master_list, axis=0).reset_index(drop=True)\n",
    "print(f'Datasets Aggregated: {len(master_list)}')\n",
    "print(f'Errors: {len(err_list)}')\n",
    "print(err_list) \n",
    "\n",
    "df_moped_master.to_csv('Merges/moped_test_merge.csv')\n",
    "df_moped_master.to_parquet('Merges/moped_test_merge.parquet', engine='auto')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c67cc6",
   "metadata": {},
   "source": [
    "## 1995-2012/MIDAS 1995-2000/94-99 ARCHFILE Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "000240d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping: 94333.zip\n",
      "Unzipping: 95018.zip\n",
      "Unzipping: 95024.zip\n",
      "Unzipping: 95038.zip\n",
      "Unzipping: 95046.zip\n",
      "Unzipping: 95053.zip\n",
      "Unzipping: 95059.zip\n",
      "Unzipping: 95066.zip\n",
      "Unzipping: 95075.zip\n",
      "Unzipping: 95082.zip\n",
      "Unzipping: 95089.zip\n",
      "Unzipping: 95094.zip\n",
      "Unzipping: 95097.zip\n",
      "Unzipping: 95101.zip\n",
      "Unzipping: 95108.zip\n",
      "Unzipping: 95109.zip\n",
      "Unzipping: 95114.zip\n",
      "Unzipping: 95117.zip\n",
      "Unzipping: 95122.zip\n",
      "Unzipping: 95129.zip\n",
      "Unzipping: 95136.zip\n",
      "Unzipping: 95164.zip\n",
      "Unzipping: 95199.zip\n",
      "Unzipping: 95228.zip\n",
      "Unzipping: 95264.zip\n",
      "Unzipping: 95296.zip\n",
      "Unzipping: 95298.zip\n",
      "Unzipping: 96016.zip\n",
      "Unzipping: 96032.zip\n",
      "Unzipping: 96037.zip\n",
      "Unzipping: 96044.zip\n",
      "Unzipping: 96052.zip\n",
      "Unzipping: 96074.zip\n",
      "Unzipping: 96086.zip\n",
      "Unzipping: 96094.zip\n",
      "Unzipping: 96109.zip\n",
      "Unzipping: 96114.zip\n",
      "Unzipping: 96122.zip\n",
      "Unzipping: 96130.zip\n",
      "Unzipping: 96164.zip\n",
      "Unzipping: 96199.zip\n",
      "Unzipping: 96226.zip\n",
      "Unzipping: 96255.zip\n",
      "Unzipping: 96290.zip\n",
      "Unzipping: 96318.zip\n",
      "Unzipping: 96352.zip\n",
      "Unzipping: 97013.zip\n",
      "Unzipping: 97028.zip\n",
      "Unzipping: 97045.zip\n",
      "Unzipping: 97050.zip\n",
      "Unzipping: 97057.zip\n",
      "Unzipping: 97065.zip\n",
      "Unzipping: 97070.zip\n",
      "Unzipping: 97076.zip\n",
      "Unzipping: 97091.zip\n",
      "Unzipping: 97100.zip\n",
      "Unzipping: 97112.zip\n",
      "Unzipping: 97120.zip\n",
      "Unzipping: 97134.zip\n",
      "Unzipping: 97161.zip\n",
      "Unzipping: 97196.zip\n",
      "Unzipping: 97217.zip\n",
      "Unzipping: 97252.zip\n",
      "Unzipping: 97280.zip\n",
      "Unzipping: 97310.zip\n",
      "Unzipping: 97343.zip\n",
      "Unzipping: 98006.zip\n",
      "Unzipping: 98020.zip\n",
      "Unzipping: 98042.zip\n",
      "Unzipping: 98049.zip\n",
      "Unzipping: 98050.zip\n",
      "Unzipping: 98057.zip\n",
      "Unzipping: 98064.zip\n",
      "Unzipping: 98071.zip\n",
      "Unzipping: 98076.zip\n",
      "Unzipping: 98086.zip\n",
      "Unzipping: 98092.zip\n",
      "Unzipping: 98099.zip\n",
      "Unzipping: 98104.zip\n",
      "Unzipping: 98111.zip\n",
      "Unzipping: 98118.zip\n",
      "Unzipping: 98125.zip\n",
      "Unzipping: 98134.zip\n",
      "Unzipping: 98139.zip\n",
      "Unzipping: 98153.zip\n",
      "Unzipping: 98168.zip\n",
      "Unzipping: 98202.zip\n",
      "Unzipping: 98223.zip\n",
      "Unzipping: 98244.zip\n",
      "Unzipping: 98288.zip\n",
      "Unzipping: 98314.zip\n",
      "Unzipping: 98342.zip\n",
      "Unzipping: 99021.zip\n",
      "Unzipping: 99041.zip\n",
      "Unzipping: 99054.zip\n",
      "Unzipping: 99068.zip\n",
      "Unzipping: 99083.zip\n",
      "Unzipping: 99096.zip\n"
     ]
    }
   ],
   "source": [
    "# Quick script to unzip all the subdirs containing the .DAT files first\n",
    "import zipfile\n",
    "\n",
    "arch_dir = 'Flow Through/1995-2012/MIDAS 1995-2000/94-99 ARCHFILE'\n",
    "#os.chdir(arch_dir)\n",
    "\n",
    "for root, dirs, files in os.walk(arch_dir):\n",
    "    for file in files:\n",
    "        file = file.lower()\n",
    "        if file.endswith('.zip'):\n",
    "            print(f'Unzipping: {file}')\n",
    "            file_name = os.path.join(root, file) # get full path of files\n",
    "            zip_ref = zipfile.ZipFile(file_name) # create zipfile object\n",
    "            zip_ref.extractall(f'{arch_dir}/_extracted') # extract file to dir\n",
    "            zip_ref.close() # close file\n",
    "            #os.remove(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "16ce578a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 94333.DAT\n",
      "Processing: 95018.DAT\n",
      "Processing: 95024.DAT\n",
      "Processing: 95038.DAT\n",
      "Processing: 95046.DAT\n",
      "Processing: 95053.DAT\n",
      "Processing: 95059.DAT\n",
      "Processing: 95066.DAT\n",
      "Processing: 95075.DAT\n",
      "Processing: 95082.DAT\n",
      "Processing: 95089.DAT\n",
      "Processing: 95094.DAT\n",
      "Processing: 95097.DAT\n",
      "Processing: 95101.DAT\n",
      "Processing: 95108.DAT\n",
      "Processing: 95109.DAT\n",
      "Processing: 95114.DAT\n",
      "Processing: 95117.DAT\n",
      "Processing: 95122.DAT\n",
      "Processing: 95129.DAT\n",
      "Processing: 95136.DAT\n",
      "Processing: 95164.DAT\n",
      "Processing: 95199.DAT\n",
      "Processing: 95228.DAT\n",
      "Processing: 95264.DAT\n",
      "Processing: 95296.DAT\n",
      "Processing: 95298.DAT\n",
      "Processing: 96016.DAT\n",
      "Processing: 96032.DAT\n",
      "Processing: 96037.DAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahernes\\AppData\\Local\\Temp\\1\\ipykernel_19124\\2867345825.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.jdate = df.apply(lambda x: convert_time_datetime(x.jdate, x.hour), axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 96044.DAT\n",
      "Processing: 96052.DAT\n",
      "Processing: 96074.DAT\n",
      "Processing: 96086.DAT\n",
      "Processing: 96094.DAT\n",
      "Processing: 96109.DAT\n",
      "Processing: 96114.DAT\n",
      "Processing: 96122.DAT\n",
      "Processing: 96130.DAT\n",
      "Processing: 96164.DAT\n",
      "Processing: 96199.DAT\n",
      "Processing: 96226.DAT\n",
      "Processing: 96255.DAT\n",
      "Processing: 96290.DAT\n",
      "Processing: 96318.DAT\n",
      "Processing: 96352.DAT\n",
      "Processing: 97013.DAT\n",
      "Processing: 97028.DAT\n",
      "Processing: 97045.DAT\n",
      "Processing: 97050.DAT\n",
      "Processing: 97057.DAT\n",
      "Processing: 97065.DAT\n",
      "Processing: 97070.DAT\n",
      "Processing: 97076.DAT\n",
      "Processing: 97091.DAT\n",
      "Processing: 97100.DAT\n",
      "Processing: 97112.DAT\n",
      "Processing: 97120.DAT\n",
      "Processing: 97134.DAT\n",
      "Processing: 97161.DAT\n",
      "Processing: 97196.DAT\n",
      "Processing: 97217.DAT\n",
      "Processing: 97252.DAT\n",
      "Processing: 97280.DAT\n",
      "Processing: 97310.DAT\n",
      "Processing: 97343.DAT\n",
      "Processing: 98006.DAT\n",
      "Processing: 98020.DAT\n",
      "Processing: 98042.DAT\n",
      "Processing: 98049.DAT\n",
      "Processing: 98050.DAT\n",
      "Processing: 98057.DAT\n",
      "Processing: 98064.DAT\n",
      "Processing: 98071.DAT\n",
      "Processing: 98076.DAT\n",
      "Processing: 98086.DAT\n",
      "Processing: 98092.DAT\n",
      "Processing: 98099.DAT\n",
      "Processing: 98104.DAT\n",
      "Processing: 98111.DAT\n",
      "Processing: 98118.DAT\n",
      "Processing: 98125.DAT\n",
      "Processing: 98134.DAT\n",
      "Processing: 98139.DAT\n",
      "Processing: 98153.DAT\n",
      "Processing: 98168.DAT\n",
      "Processing: 98202.DAT\n",
      "Processing: 98223.DAT\n",
      "Processing: 98244.DAT\n",
      "Processing: 98288.DAT\n",
      "Processing: 98314.DAT\n",
      "Processing: 98342.DAT\n",
      "Processing: 99021.DAT\n",
      "Processing: 99041.DAT\n",
      "Processing: 99054.DAT\n",
      "Processing: 99068.DAT\n",
      "Processing: 99083.DAT\n",
      "Processing: 99096.DAT\n",
      "Datasets Aggregated: 98\n",
      "Errors: 0\n"
     ]
    }
   ],
   "source": [
    "import utm\n",
    "err_list = []\n",
    "master_list = []\n",
    "file_list = []\n",
    "arch_cols = ['jdate', 'hour', 'utmn', 'utme', 'depth_ar', 'wind_spd_ar', 'wind_dir_ar', 'temp_ar', 'salinity_ar',\n",
    "             'fvolt_ar', 'nvolt_ar', 'setting_ar', 'comment_ar', 'event_ar']\n",
    "\n",
    "def convert_time_datetime(days, time):\n",
    "    # Start as defined in .MTD file\n",
    "    start = date(1899, 12, 31)\n",
    "    datetime = str(start + timedelta(days))\n",
    "    \n",
    "    hours = time\n",
    "    sec = hours * 3600\n",
    "    m, s = divmod(sec, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    time = (\"%d:%02d:%02d\" % (h, m, s))\n",
    "\n",
    "    return f'{datetime} {time}'\n",
    "\n",
    "def convert_arch_datetime(df):\n",
    "    df.jdate = df.jdate.astype(int)\n",
    "    df.hour = df.hour.astype(float)\n",
    "    df = df[df.jdate > 0]\n",
    "    df.jdate = df.apply(lambda x: convert_time_datetime(x.jdate, x.hour), axis=1)\n",
    "    df = df.rename(columns={'jdate': 'datetime'}).drop(columns=['hour'])\n",
    "    df.datetime = pd.to_datetime(df.datetime)\n",
    "    return df\n",
    "\n",
    "def convert_utm_decimal(utme, utmn):\n",
    "    # Bay Area is Zone 10 letter S according to https://upload.wikimedia.org/wikipedia/commons/b/b7/Universal_Transverse_Mercator_zones.svg\n",
    "    return utm.to_latlon(utme, utmn, 10, 'S')\n",
    "\n",
    "def convert_arch_latlon(df):\n",
    "    df.utme = df.utme.astype(float)\n",
    "    df.utmn = df.utmn.astype(float)\n",
    "\n",
    "    # Read .MTD file to understand utme/utmn conversion from km to m\n",
    "    df['temp'] = df.apply(lambda x: convert_utm_decimal((x.utme * 1000), ((x.utmn * 1000) + 4000000)), axis=1)\n",
    "    df = df.rename(columns={'utmn': 'lat', 'utme': 'lon'})\n",
    "    df[['lat', 'lon']] = df.temp.apply(pd.Series)\n",
    "    df = df.drop(columns=['temp'])\n",
    "    return df \n",
    "\n",
    "def read_arch_file():\n",
    "    count = 0\n",
    "    test_limit = math.inf\n",
    "    for root, _, files in os.walk('Flow Through/1995-2012/MIDAS 1995-2000/94-99 ARCHFILE/_extracted'):\n",
    "        for file in files:\n",
    "            if file.endswith('.DAT') and file not in file_list:\n",
    "                try:\n",
    "                    print(f'Processing: {file}')\n",
    "                    file_list.append(file)\n",
    "                    df = pd.read_csv(os.path.join(root, file), names=arch_cols, header=0)\n",
    "                    # Cleaning invalid rows from metric (not comment) columns\n",
    "                    df = df.replace('NO DATA', np.nan).replace('NODATA', np.nan).dropna(subset=arch_cols[:-3])\n",
    "                    df.comment_ar = df.comment_ar.astype(str)\n",
    "                    df.depth_ar = df.depth_ar.astype(float)\n",
    "                    df.temp_ar = df.temp_ar.astype(float)\n",
    "                    df.salinity_ar = df.salinity_ar.astype(float)\n",
    "                    df.fvolt_ar = df.fvolt_ar.astype(float)\n",
    "                    df.nvolt_ar = df.nvolt_ar.astype(float)\n",
    "                    df = convert_arch_latlon(df)\n",
    "                    df = convert_arch_datetime(df)\n",
    "                    df['dataset'] = 'ARCHFILE'\n",
    "                    df['file'] = file\n",
    "                    master_list.append(df)\n",
    "\n",
    "                    count += 1\n",
    "                    if count == test_limit: return df\n",
    "\n",
    "                except Exception as e:\n",
    "                    err_list.append(file)\n",
    "                    file_list.append(file)\n",
    "                    print(f'ERROR: {e}')\n",
    "\n",
    "read_arch_file()\n",
    "df_arch_master = pd.concat(master_list, axis=0).reset_index(drop=True)\n",
    "print(f'Datasets Aggregated: {len(master_list)}')\n",
    "print(f'Errors: {len(err_list)}') \n",
    "\n",
    "df_arch_master.to_csv('Merges/arch_test_merge.csv')\n",
    "df_arch_master.to_parquet('Merges/arch_test_merge.parquet', engine='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e41ea6b",
   "metadata": {},
   "source": [
    "## 2013-2023 Aggregation (Most Recent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ef28e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 13015dat.txt\n",
      "Processing: 13057dat.txt\n",
      "Processing: 13071dat.txt\n",
      "Processing: 13078dat.txt\n",
      "Processing: 13085dat.txt\n",
      "Processing: 13108dat.txt\n",
      "Processing: 13113dat.txt\n",
      "Processing: 13137dat.txt\n",
      "Processing: 13196dat.txt\n",
      "Processing: 13204dat.txt\n",
      "Processing: 13240dat.txt\n",
      "Processing: 13269dat.txt\n",
      "Processing: 13297dat.txt\n",
      "Processing: 13298dat.txt\n",
      "Processing: 13323dat.txt\n",
      "Processing: 13329dat.txt\n",
      "Processing: 13337dat.txt\n",
      "Processing: 14014dat.txt\n",
      "Processing: 14024dat.txt\n",
      "Processing: 14031dat.txt\n",
      "Processing: 14042dat.txt\n",
      "Processing: 14055dat.txt\n",
      "Processing: 14070dat.txt\n",
      "ERROR: 'DataFrame' object has no attribute 'datetime'\n",
      "Processing: 14084dat.txt\n",
      "Processing: 14098dat.txt\n",
      "Processing: 14105dat.txt\n",
      "Processing: 14113dat.txt\n",
      "Processing: 14127dat.txt\n",
      "Processing: 14133dat.txt\n",
      "Processing: 14157dat.txt\n",
      "Processing: 14161dat.txt\n",
      "Processing: 14189adat.txt\n",
      "Processing: 14189dat.txt\n",
      "Processing: 14196dat.txt\n",
      "Processing: 14216dat.txt\n",
      "Processing: 14224dat.txt\n",
      "Processing: 14246dat.txt\n",
      "Processing: 14259dat.txt\n",
      "Processing: 14288dat.txt\n",
      "Processing: 14307dat.txt\n",
      "Processing: 14322dat.txt\n",
      "Processing: 14344dat.txt\n",
      "Processing: 12feb2015_moped.xlsx\n",
      "Processing: 13april2015_moped.xlsx\n",
      "Processing: 13march2015_moped.xlsx\n",
      "Processing: 17march2015_moped.xlsx\n",
      "Processing: 18feb2015_moped.xlsx\n",
      "Processing: 19may2015_moped.xlsx\n",
      "Processing: 21april2015_moped.xlsx\n",
      "Processing: 22jan2015_moped.xlsx\n",
      "Processing: 27april2015_moped.xlsx\n",
      "Processing: 27march2015_moped.xlsx\n",
      "Processing: 27may2015_moped.xlsx\n",
      "Processing: 16292dat.txt\n",
      "Processing: 16313dat.txt\n",
      "Processing: 16348dat.txt\n",
      "Processing: 17011dat.txt\n",
      "Processing: 17023dat.txt\n",
      "Processing: 17039dat.txt\n",
      "Processing: 17048dat.txt\n",
      "Processing: 17066dat.txt\n",
      "Processing: 17080dat.txt\n",
      "Processing: 17094dat.txt\n",
      "Processing: 17108dat.txt\n",
      "Processing: 17117dat.txt\n",
      "Processing: 17137dat.txt\n",
      "Processing: 17157dat.txt\n",
      "Processing: 17173dat.txt\n",
      "Processing: 17206dat.txt\n",
      "Processing: 17212dat.txt\n",
      "Processing: 17234dat.txt\n",
      "Processing: 17242dat.txt\n",
      "Processing: 17262dat.txt\n",
      "Processing: 17271dat.txt\n",
      "Processing: 17291dat.txt\n",
      "Processing: 17300dat.txt\n",
      "Processing: 17318dat.txt\n",
      "Processing: 17340dat.txt\n",
      "Processing: 18010dat.txt\n",
      "Processing: 18038dat.txt\n",
      "Processing: 18039dat.txt\n",
      "Processing: 18054dat.txt\n",
      "Processing: 18068dat.txt\n",
      "Processing: 18074dat.txt\n",
      "Processing: 18085dat.txt\n",
      "Processing: 18099dat.txt\n",
      "Processing: 18108dat.txt\n",
      "Processing: 18127dat.txt\n",
      "Processing: 18137dat.txt\n",
      "Processing: 18157dat.txt\n",
      "Processing: 18162dat.txt\n",
      "Processing: 18193dat.txt\n",
      "Processing: 18201dat.txt\n",
      "Processing: 18215dat.txt\n",
      "Processing: 18226dat.txt\n",
      "Processing: 18261dat.txt\n",
      "Processing: 18262dat.txt\n",
      "Processing: 18290dat.txt\n",
      "Processing: 18299dat.txt\n",
      "Processing: 18319dat.txt\n",
      "Processing: 18339dat.txt\n",
      "Processing: 18348dat.txt\n",
      "Processing: 19030dat.txt\n",
      "Processing: 19051dat.txt\n",
      "ERROR: 'DataFrame' object has no attribute 'datetime'\n",
      "Processing: 19073dat.txt\n",
      "Processing: 19079dat.txt\n",
      "Processing: 19087dat.txt\n",
      "Processing: 19108dat.txt\n",
      "Processing: 19115dat.txt\n",
      "Processing: 19134dat.txt\n",
      "Processing: 19155dat.txt\n",
      "Processing: 19176dat.txt\n",
      "Processing: 19182dat.txt\n",
      "Processing: 19205dat.txt\n",
      "Processing: 19213dat.txt\n",
      "Processing: 19234dat.txt\n",
      "Processing: 19261dat.txt\n",
      "Processing: 19296dat.txt\n",
      "Processing: 19317dat.txt\n",
      "Processing: 19339dat.txt\n",
      "Processing: 19353dat.txt\n",
      "Processing: 20013dat.txt\n",
      "Processing: 20034dat.txt\n",
      "Processing: 20043dat.txt\n",
      "ERROR: 'DataFrame' object has no attribute 'datetime'\n",
      "Processing: 20063dat.txt\n",
      "Processing: 20195dat.txt\n",
      "Processing: 20224dat.txt\n",
      "Processing: 20254dat.txt\n",
      "Processing: 20283dat.txt\n",
      "Processing: 20324dat.txt\n",
      "Processing: 20352dat.txt\n",
      "Processing: 21050dat.txt\n",
      "Processing: 21056dat.txt\n",
      "Processing: 21081dat.txt\n",
      "Processing: 21110dat.txt\n",
      "Processing: 21138dat.txt\n",
      "Processing: 21147dat.txt\n",
      "Processing: 21168dat.txt\n",
      "Processing: 21182dat.txt\n",
      "Processing: 21211dat.txt\n",
      "Processing: 21229dat.txt\n",
      "Processing: 21243dat.txt\n",
      "Processing: 21272dat.txt\n",
      "Processing: 21300dat.txt\n",
      "Processing: 21308dat.txt\n",
      "Processing: 21340dat.txt\n",
      "Processing: 21207dat.txt\n",
      "Processing: 22025dat.txt\n",
      "Processing: 22039dat.txt\n",
      "Processing: 22068dat.txt\n",
      "ERROR: 'DataFrame' object has no attribute 'datetime'\n",
      "Processing: 22084dat.txt\n",
      "Processing: 22101dat.txt\n",
      "Processing: 22109dat.txt\n",
      "Processing: 22115dat.txt\n",
      "Processing: 22129dat.txt\n",
      "Processing: 22145dat.txt\n",
      "Processing: 22166dat.txt\n",
      "Processing: 22188dat.txt\n",
      "Processing: 22201dat.txt\n",
      "Processing: 22222dat.txt\n",
      "Processing: 22231dat.txt\n",
      "Processing: 22243dat.txt\n",
      "Processing: 22262dat.txt\n",
      "Processing: 22291dat.txt\n",
      "Processing: 22300dat.txt\n",
      "Processing: 22319dat.txt\n",
      "Processing: 23023dat.txt\n",
      "Processing: 23046dat.txt\n",
      "Processing: 23060dat.txt\n",
      "Processing: 23074dat.txt\n",
      "Processing: 23088dat.txt\n",
      "Processing: 23109adat.txt\n",
      "Processing: 23109dat.txt\n",
      "Processing: 23117dat.txt\n",
      "Datasets Aggregated: 170\n",
      "Errors: 4\n",
      "['14070dat.txt', '19051dat.txt', '20043dat.txt', '22068dat.txt']\n"
     ]
    }
   ],
   "source": [
    "err_list = []\n",
    "master_list = []\n",
    "file_list = []\n",
    "\n",
    "def convert_rec_datetime(df):\n",
    "    df = df.rename(columns={'Date': 'datetime'})\n",
    "    df.datetime = pd.to_datetime(df.datetime, format='%m/%d/%Y %H:%M:%S', errors='coerce')\n",
    "    df = df[pd.notnull(df.datetime)]\n",
    "    return df\n",
    "\n",
    "def convert_rec_latlon(df):\n",
    "    # Just reassigning lat deg and lat min (col 1 and 2) rather than creating new columns and reordering\n",
    "    df['Lat deg'] = df['Lat deg'].astype(int) + df['Lat min'].astype(float) / 60\n",
    "    df['Lat min'] = df['Lon deg'].astype(int) + df['Lon min'].astype(float) / 60\n",
    "    df = df.drop(columns=['Lon deg', 'Lon min']).rename(columns={'Lat deg': 'lat', 'Lat min': 'lon'})\n",
    "    return df\n",
    "\n",
    "def read_rec_file():\n",
    "    count = 0\n",
    "    test_limit = math.inf\n",
    "    for root, _, files in os.walk('Flow Through/Recent'):\n",
    "        for file in files:\n",
    "            file = file.lower()\n",
    "            if file.endswith('dat.txt') and file not in file_list and \"log\" not in file:\n",
    "                try:\n",
    "                    print(f'Processing: {file}')\n",
    "                    file_list.append(file)\n",
    "                    df = pd.read_csv(os.path.join(root, file))\n",
    "                    df = convert_rec_datetime(df)\n",
    "                    df = convert_rec_latlon(df)\n",
    "                    df['dataset'] = 'REC'\n",
    "                    df['file'] = file\n",
    "                    master_list.append(df)\n",
    "\n",
    "                    count += 1\n",
    "                    if count == test_limit: return\n",
    "                except Exception as e:\n",
    "                    err_list.append(file)\n",
    "                    file_list.append(file)\n",
    "                    print(f'ERROR: {e}')\n",
    "\n",
    "            # 2015 has no dat.txt files, only .xlsx\n",
    "            if file.endswith('.xlsx') and os.path.exists(f'Flow Through/Recent/2015/{file}'):\n",
    "                try:\n",
    "                    print(f'Processing: {file}')\n",
    "                    file_list.append(file)\n",
    "                    df = pd.read_excel(os.path.join(root, file)).drop(columns=['UTM North', 'UTM East'])\n",
    "                    df = convert_rec_datetime(df)\n",
    "                    df = convert_rec_latlon(df)\n",
    "                    df['dataset'] = 'REC'\n",
    "                    df['file'] = file\n",
    "                    master_list.append(df)\n",
    "\n",
    "                    count += 1\n",
    "                    if count == test_limit: return\n",
    "\n",
    "                except Exception as e:\n",
    "                    err_list.append(file)\n",
    "                    file_list.append(file)\n",
    "                    print(f'ERROR: {e}')\n",
    "                \n",
    "\n",
    "\n",
    "read_rec_file()\n",
    "df_rec_master = pd.concat(master_list, axis=0).reset_index(drop=True).drop(columns=['UTM North', 'UTM East'])\n",
    "df_rec_master['dataset'] = '2013-2023'\n",
    "\n",
    "df_rec_master.Chlor = df_rec_master.Chlor.combine_first(df_rec_master['Chlor (volts)'])\n",
    "df_rec_master.Chlor = df_rec_master.Chlor.combine_first(df_rec_master['Chloro (volts)'])\n",
    "df_rec_master.Turb = df_rec_master.Turb.combine_first(df_rec_master['Turb (volts)'])\n",
    "#df_rec_master['Air Temp'] = df_rec_master['Air Temp'].combine_first(df_rec_master.Temp)\n",
    "\n",
    "print(f'Datasets Aggregated: {len(master_list)}')\n",
    "print(f'Errors: {len(err_list)}')\n",
    "print(err_list) \n",
    "\n",
    "df_rec_master.to_csv('Merges/rec_test_merge.csv')\n",
    "df_rec_master.to_parquet('Merges/rec_test_merge.parquet', engine='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69fbb29",
   "metadata": {},
   "source": [
    "## Master Aggregation\n",
    "Aggregate 4 subdatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12481538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_arch = pd.read_parquet('Merges/arch_test_merge.parquet')\n",
    "df_midas = pd.read_parquet('Merges/midas_test_merge.parquet')\n",
    "df_moped = pd.read_parquet('Merges/moped_test_merge.parquet')\n",
    "df_rec = pd.read_parquet('Merges/rec_test_merge.parquet')\n",
    "\n",
    "length = len(df_midas) + len(df_moped) + len(df_rec)\n",
    "order = ['datetime', 'lat', 'lon', 'depth', 'photic_depth', 'wind_spd', 'wind_dir', 'water_temp', 'air_temp', 'bow_temp',\n",
    "         'barometer', 'chlor', 'turbidity', 'suspended_part', 'salinity', 'fluor_switch', 'comment', 'mark', 'dataset', 'file']\n",
    "\n",
    "# prep df_arch\n",
    "#df_arch = df_arch.rename(columns={'depth_ar': 'depth',  'wind_spd_ar': 'wind_spd', 'wind_dir_ar': 'wind_dir', \n",
    "#                                  'temp_ar': 'water_temp', 'salinity_ar': 'salinity', 'fvolt_ar' : 'chlor', 'nvolt_ar': 'turbidity',\n",
    "#                                  'setting_ar': 'fluor_switch', 'comment_ar': 'comment', 'event_ar': 'mark'})\n",
    "\n",
    "#print(df_arch.head())\n",
    "\n",
    "# prep df_midas\n",
    "df_midas = df_midas.rename(columns={'depth_md': 'depth', 'temp_air_md': 'air_temp', 'wind_spd_md': 'wind_spd',\n",
    "                                    'wind_dir_md': 'wind_dir', 'barometer_md': 'barometer', 'bow_temp_md': 'bow_temp',\n",
    "                                    'salinity_md': 'salinity', 'sus_part_matter_md': 'suspended_part',\n",
    "                                    'photic_depth_md': 'photic_depth', 'chlor_md': 'chlor', 'neph_md': 'turbidity',\n",
    "                                    'fluor_switch_md': 'fluor_switch', 'comment_num_md': 'comment', 'event_mark_md': 'mark'})\n",
    "\n",
    "#print(df_midas.head())\n",
    "\n",
    "# prep df_moped\n",
    "df_moped = df_moped.drop(columns=['heading_mag_mp', 'heading_gps_mp', 'speed_gps_mp', 'speed_water_mp', 'spare_mp'])\n",
    "df_moped = df_moped.rename(columns={'depth_mp': 'depth', 'speed_wind_mp': 'wind_spd', 'wind_dir_mp': 'wind_dir',\n",
    "                                    'temp_water_mp': 'water_temp', 'temp_air_mp': 'air_temp', 'barometer_mp': 'barometer', 'chlor_mp': 'chlor',\n",
    "                                    'turb_mp': 'turbidity', 'salinity_mp': 'salinity', 'temp_bow_mp': 'bow_temp', 'mark_mp': 'mark'})\n",
    "\n",
    "#print(df_moped.head())\n",
    "\n",
    "\n",
    "# prep df_rec\n",
    "df_rec = df_rec.drop(columns=['Head mag', 'Head Gps', 'Spd Gps', 'Spd Wat', 'Spare', 'Chlor (volts)', 'Turb (volts)',\n",
    "                              'Chloro (volts)', 'Scufa Fluor', 'Temp', 'PF Fo', 'PF Fm', 'PF blank', 'PF Fv', 'PF yield'])\n",
    "df_rec = df_rec.rename(columns={'Depth': 'depth', 'Spd Wind': 'wind_spd', 'Wind dir': 'wind_dir', 'Air Temp': 'air_temp',\n",
    "                                'Water Temp': 'water_temp', 'Sal': 'salinity', 'Chlor': 'chlor', 'Turb': 'turbidity',\n",
    "                                'Mark': 'mark', 'Bow Temp': 'bow_temp', 'Baro': 'barometer'})\n",
    "\n",
    "#print(df_rec.head())\n",
    "\n",
    "df_master = (pd.concat([df_midas, df_moped, df_rec], ignore_index=True)\n",
    "            .reset_index(drop=True)\n",
    "            .reindex(columns=order)\n",
    "            .sort_values(by='datetime'))\n",
    "\n",
    "df_master.comment = df_master.comment.astype(str)\n",
    "df_master.mark = df_master.mark.astype(str)\n",
    "\n",
    "assert(length == len(df_master))\n",
    "#print(df_master.columns)\n",
    "#print(df_master)\n",
    "\n",
    "df_master.to_parquet('Merges/PETERSON_MASTER.parquet', engine='auto')\n",
    "df_master.to_csv('Merges/PETERSON_MASTER.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98fcee6",
   "metadata": {},
   "source": [
    "## Peterson Master Dataset Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5397889f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PQ file size: 90.67mb\n",
      "Total Samples: 2495711\n",
      "Earliest Sample 1988-01-09 11:11:46\n",
      "Latest Sample: 2023-04-27 16:27:57\n",
      "Datetime Dupes: \n",
      "datetime\n",
      "2021-12-06 11:55:32    13\n",
      "2021-12-06 11:55:36    10\n",
      "2021-12-06 11:55:12    10\n",
      "2021-12-06 11:55:53     9\n",
      "2021-12-06 11:55:24     8\n",
      "                       ..\n",
      "2003-05-01 11:58:03     1\n",
      "2003-05-01 11:58:08     1\n",
      "2003-05-01 11:58:13     1\n",
      "2003-05-01 11:58:18     1\n",
      "2023-04-27 16:27:57     1\n",
      "Name: count, Length: 2489659, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('Merges/PETERSON_MASTER.parquet')\n",
    "print(f'PQ file size: {round((os.path.getsize(\"Merges/PETERSON_MASTER.parquet\") / (1024*1024)), 2)}mb')\n",
    "print(f'Total Samples: {len(df)}')\n",
    "print(f'Earliest Sample {df.datetime.min()}')\n",
    "print(f'Latest Sample: {df.datetime.max()}')\n",
    "print(f'Datetime Dupes: \\n{df.groupby(\"datetime\").datetime.value_counts().sort_values(ascending=False)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
